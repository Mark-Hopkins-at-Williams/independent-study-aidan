# Encoder-Decoder Transformer Training/Finetuning

This repository contains code for training/finetuning encoder-decoder transformer models for machine translation.

## Finetuning

You can run a quick example finetuning with the following command:

    python finetune.py --config examples/example1.json

It should create a new directory called ```examples/model-example1-v0```. After completion of the finetuning, the directory will contain the trained model, as well as the following files:

- `training.png`: Plot of the training and validation losses.
- `translations.json`: Stores the test set translations for each language pair specified in the config.
- `scores.json`: Stores the BLEU and ChrF scores of these translations with respect to the references in the test set.

The finetuning is driven by the JSON configuration file (in this case, `examples/example1.json`):

    {
        "model_dir": "examples/model-example1",
        "finetuning_parameters": {
            "base_model": "facebook/nllb-200-distilled-600M",
            "finetune": false,
            "freeze_encoder": false,
            "freeze_decoder": false,
            "batch_size": 32,
            "num_steps": 2000
        },
        "corpora": {
            "europarl": {
                "eng": {
                    "lang_code": "eng_Latn",
                    "train": "examples/data/train.en",
                    "dev": "examples/data/dev.en",
                    "test": "examples/data/test.en",
                    "permutation": 0
                },
                "tsn": {
                    "lang_code": "tsn_Latn",
                    "train": "examples/data/train.es",
                    "dev": "examples/data/dev.es",
                    "test": "examples/data/test.es",
                    "permutation": 1
                },
                "tso": {
                    "lang_code": "tso_Latn",
                    "train": "examples/data/train.es",
                    "dev": "examples/data/dev.es",
                    "test": "examples/data/test.es",
                    "permutation": 2
                }
            }
        },
        "bitexts": [
            {
                "corpus": "europarl",
                "src": "eng",
                "tgt": "tsn",
                "train_lines": [
                    0,
                    1024
                ]
            },
            {
                "corpus": "europarl",
                "src": "eng",
                "tgt": "tso",
                "train_lines": [
                    1024,
                    2048
                ]
            }
        ]
    }

Things of note:

- The base model should either be a path to a saved model, or the name of a model recognized by `transformers.AutoTokenizer`.
- The files in `"corpora"` should be represented in plain text, one segment per line. It is assumed that each `train` file (respectively, `dev` and `test`) has the same number of lines, and that line `k` corresponds to same meaning in each.
- Specifying permutation 0 means that no encipherment will occur. Any other permutation will randomly permute the tokens generated by the tokenizer. If you request the same permutation for two corpora, it will use the same permutation to encipher the tokens for each corpus.
- The language codes (e.g. `eng_Latn`, `tsn_Latn`, `tso_Latn`) are the language ids that will be prepended by the tokenizer to the tokenized text for that corpus. Note that we do not use `esp_Latn` in this example because we are enciphering the two Spanish corpora.

